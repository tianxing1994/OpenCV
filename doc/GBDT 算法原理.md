### GBDT 算法原理
#### Sigmoid 函数
$$f(x) = \frac{1}{1 + e^{-x}}$$
求 sigmoid 函数的一阶导数: 
$$f'(x) = -1 {\times} \frac{1}{(1 + e^{-x})^{2}} {\times} e^{-x} {\times} (-1) = \frac{e^{-x}}{(1 + e^{-x})^{2}}$$
已知: $1 - f(x) = \frac{e^{-x}}{1 + e^{-x}}$
则: 
$$ f'(x) =  \frac{1}{1 + e^{-x}} {\times} \frac{e^{-x}}{1 + e^{-x}} = \frac{e^{-x}}{(1 + e^{-x})^{2}} = f(x) {\times} (1 - f(x))$$

#### GBDT 算法残差推导
当前模型 m 预测出的值 $F_{m}(X)$ 应等于前一模型 m-1 预测出的值 $F_{m-1}(X)$ 加一个增量 f_{m}(X)$.
$$z = F_{m}(X) = F_{m-1}(X) + f_{m}(X)$$
其中 X 是样本, F(X) 函数是用于预测残差的模型.
现在我们需要求 $f_{m}(X)$ 函数. 
有: 
$$P = \frac{1}{1 + e^{-z}}$$
其中: P 是当前模型预测出的 z 值代入 sigmoid 函数得到的概率值. 
##### 二分类问题的似然函数
假设有一组样本 X, 其中有 m 个 1 或 0 的值. 我们用 $y_{i}$ 来代表样本中的每一个值. 用 $p$ 来代表预知的样本出现 1 的概率. 则当前样本发生的概率为: 
$$Likelihood(p, y) = 
\prod_{i=0}^m{p^{y_{i}} {\times} (1 - p)^{(1-y_{i})}}$$

##### 取极大似然函数的对数作为损失函数
$$Loss(y, F(X)) = 
\sum_{i=1}^m{(y_{i}logp + (1 - y_{i})log(1-p)))}$$
损失函数对 z 求导, 即求上面的 $F_{m}(X)$ 求导.
**一阶导数**为: 
$$
Loss'(y, F(X)) = 
$$

$$
\sum_{i=1}^m{(y_{i} {\times} \frac{1}{p} {\times} p {\times} (1 - p) - ((1-y_{i}) {\times} \frac{1}{1-p} {\times} p {\times} (1 - p))} =
$$

$$
\sum_{i=1}^m{y_{i}(1 - p) - (1 - y_{i})p} = 
$$

$$
\sum_{i=1}^m{(y_{i} - p)}
$$

即: 
$$Loss'(y, F(X)) = \sum_{i=1}^m{(y_{i} - p)}$$

**二阶导数**为: 
$$Loss''(y, F(X)) = - \sum_{i=1}^m{p(1 - p)} = \sum_{i=1}^m{p(p - 1)}$$

##### 已知泰勒公式的一阶展开
$$f(x + x_{0}) = f(x) + f'(x) {\times} x_{0}$$
令 $f(x) = g'(x)$, 则: 
$$g'(x + x_{0}) = g'(x) + g''(x) {\times} x_{0}$$

##### 求解 $f_{m}(X)$
已知: 
$$z_{m} = F_{m}(X) = F_{m-1}(X) + f_{m}(X) = z_{m-1} + {\delta}z$$
则损失函数对 z 求导, 有: 
$$Loss'(y, F_{m}(X)) = Loss'(y, F_{m-1}(X) + f_{m}(X))$$
把 $f_{m}(X)$ 看作 $x_{0}$, 并结合泰勒展开式. 则: 
$$Loss'(y, F_{m}(X)) = Loss'(y, F_{m-1}(X) +  Loss''(y, F_{m-1}(X)) {\times} f_{m}(X)$$
令损失函数的一阶导数为 0, 即: 
$$f_{m}(X) = - Loss'(y, F_{m-1}(X)) / Loss''(y, F_{m-1}(X))$$
损失函数的一阶, 二阶导数已知, 有: 
$$f_{m}(X) = \frac{\sum_{i=1}^m{}(y_{i} - p)}{\sum_{i=1}^m{p}(1 - p)}$$

由此得出, 残差增量 ${\delta}z$
其中 $p$ 为 $F_{m-1}(X)$ 得出的 $z_{m-1}$ 值代入 Sigmoid 函数得出的概率值: 
$$p = \frac{1}{1 + e^{-z_{m-1}}}$$

##### 注意事项
* 需要注意的是, 在此残差推导过程中所用的样本数量 m 并不是模型训练时所时用的整个样本集的样本数量. 
在 GBDT 所使用的回归树算法中, 由于它是通过不断地细分叶子节点来做回归, 而不是通过拟合一个数学公式, 所以不论怎么细分叶子节点, 都不能排除同一个叶子节点中包含有多个样本且样本的真实分类也不一定相同. 
此处残差推导中所用的 m 就是指的同一个叶子节点中的样本数量, 因为我们认为同一个叶子节点中的样本为 1 或者为 0 的概率 $p_{1}$ 是一样的, 我们最大化似然函数求解 $p_{1}$, GBDT 算法便是迭代地求解该 $p_{1}$. 