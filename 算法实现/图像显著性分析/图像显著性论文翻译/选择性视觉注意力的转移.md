### [选择性视觉注意力的转移: 向潜在的神经回路转移](https://cseweb.ucsd.edu/classes/fa09/cse258a/papers/koch-ullman-1985.pdf)
C. Koch and S. Ullman

人工智能实验室和生物信息处理中, MIT, E 25-201, 剑桥, MA, 02139, USA,
和应用数学系, 魏茨曼科学研究所, 雷霍活特 76100, 以色列,
和心理学系, MIT, 剑桥, MA, USA.

#### 摘要
心理和生理学证据表明, 灵长类动物和人类的视觉系统已经进化出了专门在视觉场景中扫视以获取重点信息的能力.
这项研究解决了这样一个问题, 即简单的类神经元网络是怎样通过选择性视觉注意力转移实现对特殊事物的搜索的.

具体地, 我们提出以下观点:
1. 许多基本特征, 如: 颜色, 方向, 运动方向, 视差等, 在不同的地形图中并行表示, 称为早期表示.
2. 存在从早期地形图表示到更加中心化的非地形图的表示的选择性映射.
如此, 在任何时候, 中心化表示都包含视觉场景中单个位置的属性, 那个被选中的位置.
我们认为, 这种映射是早期选择性视觉注意的主要表达. 选择性注意的功能之一是将来自不同地图的信息融合为一个连贯的整体.
3. 某些选择性规则会决定将哪些位置映射到中心表示中. 其主要规则是通过利用早期表示中的显眼位置, 及所谓的内部通吃网络实现.
在此网络中屏蔽掉被选中的位置, 会导致其自动转移到下一个更可疑的位置. 其它的次要规则根据接近性和相似性偏好来定义.

我们讨论了如何在类神经元网络中实现这些规则, 并提出了从视觉皮层到 LGN 的粗略的反向投影的可能任务.
关于视野中物体的检测, 定位和识别的许多心理物理学研究提出了人类视觉感知的两步理论.
1. 第一步, 是 "预注意" 模式, 在这种模式下, 简单的特征可以在整个视野中快速并行地处理.
2. 第二步, "注意" 模式, 专门处理焦点信息, 通常称为关注焦点, 针对视野中的特定位置进行处理.
复杂形式的分析和物体的识别与第二阶段有关(Neisser 1967; Bergen and Julesz 1983; Treisman 1983; Ullman 1984; Julesz 1984).
对于这种假设的计算依据来自于以下认识:
虽然可以想象在特定位置执行诸如形状分析和识别之类的任务的特定算法, 但是很难想象这些算法在可见的视觉场景上并行动行,
因为这种方法很快会导致所需计算资源的组合爆炸 (Poggio 1984; Ullman 1984).
这基本上是对 Minsky 和 Papert 在视觉感知中的通用应用的主要批评 (Minsky and Papert 1969).
综合来看, 这些经验和理论研究表明, 在特定的预处理阶段之后, 对视觉信息的分析是按操作顺序进行的,
每个操作都应用于选定的一个或多个位置.


"选择性注意" 的实验证据主要来自心理物理学和生理学这两个不同的来源.
可以围绕视觉场景移动, 与场景相关但不同的专门处理焦点的心理物理学证据可以分为两类实验.
首先, Treisman 及其合作者的实验 (Treisman 和 Gelade 1980; Treisman 1982, 1983) 表明,
对由单个特征定义的目标进行视觉搜索 (例如: 在许多红色的线中寻找绿色的线) 在空间显示中并行发生,
而对由多个特征定义的联合目标进行搜索 (例如: 在可能是红色或蓝色, 水平或垂直的线中搜索红色和垂直线),
则需要进行连续的, 自终止的扫描分散在展示空间中的物体.
因此, 虽然对单个特征定义的目标进行搜索时, 干扰物数量曲线基本是平坦的, 但对多个特征定义的目标进行搜索时, 它是线性增加的.
Julesz 在他对纹理特征的辨别的研究中还表明, 只有有随的特征表示的纹理才可以被并行处理 (Bergen and Julesz 1983; Julesz 1984).
可以并行检测的特征包括颜色, 线段和方向以及某些形状参数, 例如:
曲率 (Treisman 1983; Julesz and Bergen 1983) 和可能的立体感 (Nielsen and Poggio, unpublished experiments).
其次, 在一系列不同的实验中, 其要求受试者检测给定的目标, 并给出有并目标预期位置的提示.
因此, 受试者 "注视了" 预期的位置, 而没有注视和专注 (因为他们被要求始终注视一个测试点).
该表现通常优于没有预提示建议其将处理焦点提前转移到特定的空间位置
(Eriksen and Hoffman 1972; Posner 1980; Bashinkski and Bacharach 1980; Remington and Pierce 1984) 的情况.


在生理学研究中还发现了与视觉信息选择性处理有关的现象.
在一系列对醒着的, 表现良好的猴子的记录中, 戈德堡和维尔茨发现, 如果猴子打算将其感受野用作快速眼球运动(扫视)的目标,
那么上丘浅层表层细胞对视觉刺激的反应就会增强.
这些细胞本身对眼睛的运动没有反应, 因为在黑暗中进行扫视时, 它们没有放电.
这些神经元的放电与眼跳运动无关.
该效果具有高度的空间选择性, 因为扫视视野的其他区域不会引起这种增强.
在顶叶反皮质 (Brodman's area 7), Bushnell et all. (1981) 观察到, 只要动物出于某种行为(例如出手触摸刺激, 向刺激发起眼动等),
只要动物使用被研究神经元的感受野, 视尝反应就会增强.
同样, 这种效应是空间选择性的 (see also Mountcastle et al. 1981).
最近 haenny 等.(1984) 证明了猴子的 V4 区域的选择性门控.
在他们的实验中, 如果猴子检测到触觉和视觉刺激之间的一致 (如果刻度盘上的线槽放向与视觉呈现的光栅的方向平行), 则需要释放表盘.
而某些细胞则对特定的视觉提示有反应, 而与触觉无关, 例如: 他们总是对水平光栅做出反应, 只有当两个图案的方向没有差异时才放电.
总而言之, 视觉系统中的某些单个细胞对相同的物理刺激的反应不同,
从而根据执行的视觉任务增强它们的反应 (see also Moran and Desimone 1985).

这些结果以及可以对选事实上位置进行视觉分析的概念引发了许多有趣的问题. 视觉系统可以对选定位置执行哪些操作 ?
选择操作是怎样处理的 ? 也就是说, 什么决定了下一个要处理的位置, 处理如何从当前位置转移到下一个所选位置 ?
在本文中, 我们将首先定义问题, 然后研究可能的机制及其在简单类神经元网络中的实现, 以控讨其中的一些问题.


#### 问题
现在, 我们将着手为以后的讨论建立总体框架, 强调我们的假设以及所考虑的问题的确切性质.
作为起点, 我们将提出一个框架, 用于讨论细胞生理学方面的选择性注意机制.
实验证据表明, 选择性视觉注意已经在早期加工阶段发挥了重要作用, 因此, 尝试将选择性注意与生理水平联系起来是合理的.
我们假设选择性的视觉注意力作用于我们所说的早期表示, 即: 视觉环境被编码为一组地形图, 皮质图 (Zeki 1978; Barlow 1981).
早期表示包含针对不同基本特征的各种不同地图, 例如边缘的方向, 颜色, 视差和运动方向.
对于这些地图中的每个位置, 都有许多维度, 如: 不同的颜色或方向.
这些地图中保留了邻域关系, 即视觉场景中的附近位置投影到了地图中的附近位置.
局部抑制性连接调节侧向抑制, 发生在较早的阶段或要素图中.
因此, 在此阶段会选出那些与其周围信号明显不一样的位置.

因此, 每张地图的状态都表明视觉场景中给定位置的明显程度: 被相似的红色斑点包围的红色斑点不如被绿色斑点包围的红色斑点明显.
应该强调的是, 不同的地图不一定必须在物理上不同的位置, 而是可以混合在一起的.
此外, 根据多个空间通道的证据, 这些地图可能以不同的比例尺度存在, 即以不同的空间辨率存在 (Campbell and Robson 1968; Wilson and Bergen 1979).
当观察者有选择地注意某个特定位置时, 与所选位置关联的属性将被映射为更高, 更抽象的非地形表示形式.
理解视觉场景中所选位置的属性等同于理解非地形表示中的属性.
该框架在总体上与致力于处理不同特征的皮质区域层次结构的概念兼容.

在这个框架下, 我们现在可以问一些有关选择性视觉注意的操作的具体问题.
这里, 至少有两个问题需要解决.
1. 赢者通吃问题: 确保每个地图中只有一个位置处于活跃状态, 而最初处于活跃状态的位置却很多.
2. 空间配准问题: 就是将不同的特征图彼此对齐. 结合并对齐不同特征图的信息对检测与单个位置有关的信息提供了快速可靠的途径.
该途径的解剖学联系在哪里 ?
3. 转移问题: 处理焦点如何转移到另一个位置 ?
在下文中, 我们提出了这些问题的一些答案.


##### 有利于选择性视觉注意力的两种机制.
为了了解, 选择性映射对可选位置属性的转换是怎样工作的, 我们将介绍两种直观上合理的机制;
一种方法是对视觉场景中某个位置的明显程序进行简单的度量, 另一种方法是从大量的活跃单位中选择最活跃的单位.
使用这种机制而不是高级认知概念的语言来描述选择性注意的操作, 其优点是可以得出关于涉及注意的特定皮质区域的解剖和电生理的特殊预测.
我们需要指出, 我们并不是需要提出一种与大脑的动行机制完全相同的机制.
而是一种能够兼容皮层生理学和解剖学的有简单机制来解释选择性视觉注意力和相关视觉操作的转移.

###### 显著图
给定不同的基本特片图,
这似乎是合理的, 在视觉场景中某个位置的显眼性决定了不同的其本特征图中相应位置的活跃程度.
他们的活跃程度越高(如, 发射频率), 则视野中相应位置的显著性就越高.
因此, 不同的特征图表示不同维度内的显著性.
为了评估某个位置的整体全局显著性, 我们将假设存在另一个地形图, 即显著图, 该图将各个地图的信息组合成一个全局部显著性度量. 
基本特征图中的一个位置对应的点投影到显著图中的一个单元上. 
只要在特征图中的显着性增加与在显着图中的显着性增加相对应, 投影的确切性质就在这里不相关. 
然后, 该图会给出视觉环境的 "偏见" 视图, 强调视野中有趣或显眼的位置. 
由于显著性图仍是早期视觉系统的一部分, 因此很可能以简单的属性 (如: 颜色, 运动方向, 深度和方向) 编码对象的显眼性. 
给定位置的显著性由该位置与其周围的颜色, 方向, 运动, 深度等之间的差异程度决定. 
然而, 有可能, 可以通过一些较高的皮层中心的活动来调节不同属性的相对权重, 
例如, 在长时间练习某些特定目标和干扰因素时 (Schneider and Shiffrin 1977). 

###### 选择性图
接下来, 我们必须确保仅将与最显眼位置相对应的属性从早期表示映射到更中心的位置. 
因此, 我们假设一个 "开关" 可以将单个位置 (选定或引人注意的位置) 的属性路由到中心表示中. 
注意, 从视觉输入中提取某些属性所需的计算是在早期表示内进行的, 即在选择过程之前, 而不是在选择过程之后. 
这种区别很重要, 例如在颜色计算中. 
正如心理物理学所证明的那样, 颜色感知的计算是一个全局过程, 需要整个(或很大一部分)视野.
因此, 有理由假设在选择要进行进一步处理的位置之前, 颜色和其他属性的计算是在早期表示中进行的. 
这种关注的观点是根据这样的观点得出的, 即不可能将视觉关注同时分配给空间中的两个不同位置 (Posner et al. 1980). 

其本机制. 信息从早期表示到中央表示的选择性路由的基础操作可以由两个互补的蜂窝网络执行. 
这样的一个网络被称为 "赢者通吃" (WTA netwrk; see Feldman 1982, who introduced this term; 
Feldman and Ballard 1982)网络, 在显著图中定位了最活跃的单位, 而第二个网络将所选位置的属性中继到中心表示. 
在任何给定时间, 仅从早期表示中选择一个位置并将其复制到中央表示中. 
WTA 网络等效于最大发现操作器, 在显著图中操作单位的输出 x_{i}
在神经网络中, x_{i} 可以解释为单元在位置 i 的电活动 (细胞内电压或峰值速率). 
WTA 机制使用转换规则将这组输入单元映射到由 y 表示的相等数量的输出单元上. 

```angular2html
y_{i} = 0           if x_{i} < max(x_{i})
y_{i} = f(x_{i})    if x_{i} = max(x_{i})
```
其中 f 是 x_{i} 的增函数 (包含常量). 除了活跃度最大的相关输入单元, 所有的输出都为 0. 


"赢家通吃" 的网络. 建立 WTA 网络可能看起来很简单, 但是当考虑到生物硬件的内在特性时, 就会出现复杂性. 
根据底层硬件的不同, 可以考虑两个极端来计算给定集合的最大值. 
在串行计算机上, 最简单的算法是在整个输入集中按顺序搜索最大的数字. 
该方法的缺点是对于 n 个输入, 则需要 n 个基本时间长度 (基本时间步长, 我们总是指执行基本操作所需要时间). 
一台具有 n 个处理器的高度并行的计算机, 每个计算机都可以直接访问其它 n-1 个处理器, 
可以通过同时比较每个处理器的值与所有其他处理器的值来在一个时间步长内计算最大值. 

一种简单的实现可能是由 Hadeler(1974) 研究的那种相互抑制的网络, 其中每个单元都抑制其他每个单元. 
在这些网络中, 假定神经元是线性求和设备, 然后进行阈值运算 (see for instance McCulloch and Pitts 1943). 
但是, 这样的网络将无法对任意输入 x_{i} 实施 WTA 计算, 因为没有保证网络会收敛 (for more details, see Koch and Ullman 1984). 
此外, 就所需要的连接总数而言, 这些网络中的每个单元都必须与其他每个单元连接的要求似乎是过高的 (n^2 - n 如果连接是单向的). 
我们基于两个生物学动机的假设, 提出了一种更可行的 WTA 网络实施方案. 
1. 除了某些远程的兴奋性连接以外, 大多数连接 (无论是兴奋的还是抑制的) 都是局部的. 
2. 每个基本处理单元仅执行一些简单指定的操作, 例如加法或乘法. 特别地, 基本处理单元不能使用任何符号信息, 例如地址. 在第一个网络中, 
每个单位 i 都与变量 y 相关联. 每个单元接收一个恒定且非负的输入 x_{i}. y_{i} 的状态方程为: 
![equation](http://chart.googleapis.com/chart?cht=tx&chl=\Large\frac{dy_{i}}{dt}%20=%20y_{i}%20(x_{i}%20-%20\sum{x_{j}%20y_{j}}))

其中, 从 1 到 n 对所有的 j 进行累加. 
该方程本身由 K.P.Hadeler 提取. 其初始参数: 
![equation](http://chart.googleapis.com/chart?cht=tx&chl=\Large\sum_{j}{y_{j}(0)}%20=%201)
其解由下式得出: 
![equation](http://chart.googleapis.com/chart?cht=tx&chl=\Large%20y_{t}(t)%20=%20\frac{y_{i}(0)e^{x}t^{t}}{\sum_{j}{y_{j}(0)e^{x}j^{t}}})

通过检测该方和式, 我们可以立即看到, 如果 x_{i} 是所有 x_{j} 中的最大值, 则对应的 y_{i} 会渐近趋于 1, 而所有其他 y_{j} 都趋于 0. 
形式上, y_{i} 对应于离散概率分布, 因为在所有时间 t 都有 
![equation](http://chart.googleapis.com/chart?cht=tx&chl=\Large\sum_{j}y_{j}(t)%20=%201). 
然后 
![equation](http://chart.googleapis.com/chart?cht=tx&chl=\Large\sum_{j}y_{j}(t))
 对应于网络的平均活动. 
注意, y_{i} 的收敛速度取决于输入 x_{i} 的强度. 
对于大输入, 时间常数 1/x_{i} 小且收敛迅速, 而对于 x_{i} 稍大于 x_{j} 的情况, 收敛慢. 

未完, 太多了, 翻译好麻烦. 























