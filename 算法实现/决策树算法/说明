参考链接:
https://www.cnblogs.com/sxron/p/5471078.html

对以下内容有感, 保存下来.

1、信息增益比选择最佳特征
　　以信息增益进行分类决策时，存在偏向于取值较多的特征的问题。于是为了解决这个问题人们有开发了基于信息增益比的分类决策方法，也就是C4.5。C4.5与ID3都是利用贪心算法进行求解，不同的是分类决策的依据不同。

　　因此，C4.5算法在结构与递归上与ID3完全相同，区别就在于选取决断特征时选择信息增益比最大的。

　　信息增益比率度量是用ID3算法中的的增益度量Gain(D，X)和分裂信息度量SplitInformation(D，X)来共同定义的。分裂信息度量SplitInformation(D，X）就相当于特征X（取值为x1，x2，……，xn，各自的概率为P1，P2，...，Pn，Pk就是样本空间中特征X取值为xk的数量除上该样本空间总数）的熵。

　　SplitInformation(D，X） = -P1 log2(P1)-P2 log2(P)-,...,-Pn log2(Pn)

　　GainRatio(D,X) = Gain(D,X)/SplitInformation(D,X)

　　在ID3中用信息增益选择属性时偏向于选择分枝比较多的属性值，即取值多的属性，在C4.5中由于除以SplitInformation(D,X)=H(X)，可以削弱这种作用。

2、处理连续数值型特征
　　C4.5既可以处理离散型属性，也可以处理连续性属性。在选择某节点上的分枝属性时，对于离散型描述属性，C4.5的处理方法与ID3相同。对于连续分布的特征，其处理方法是：

　　先把连续属性转换为离散属性再进行处理。虽然本质上属性的取值是连续的，但对于有限的采样数据它是离散的，如果有N条样本，那么我们有N-1种离散化的方法：<=vj的分到左子树，>vj的分到右子树。计算这N-1种情况下最大的信息增益率。另外，对于连续属性先进行排序（升序），只有在决策属性（即分类发生了变化）发生改变的地方才需要切开，这可以显著减少运算量。经证明，在决定连续特征的分界点时采用增益这个指标（因为若采用增益率，splittedinfo影响分裂点信息度量准确性，若某分界点恰好将连续特征分成数目相等的两部分时其抑制作用最大），而选择属性的时候才使用增益率这个指标能选择出最佳分类特征。

　　在C4.5中，对连续属性的处理如下：

　　　　1、对特征的取值进行升序排序

　　　　2、两个特征取值之间的中点作为可能的分裂点，将数据集分成两部分，计算每个可能的分裂点的信息增益（InforGain）。优化算法就是只计算分类属性发生改变的那些特征取值。

　　　　3、选择修正后信息增益(InforGain)最大的分裂点作为该特征的最佳分裂点

　　　　4、计算最佳分裂点的信息增益率（Gain Ratio）作为特征的Gain Ratio。注意，此处需对最佳分裂点的信息增益进行修正：减去log2(N-1)/|D|（N是连续特征的取值个数，D是训练数据数目，此修正的原因在于：当离散属性和连续属性并存时，C4.5算法倾向于选择连续特征做最佳树分裂点）

3、叶子裁剪
　　分析分类回归树的递归建树过程，不难发现它实质上存在着一个数据过度拟合问题。在决策树构造时，由于训练数据中的噪音或孤立点，许多分枝反映的是训练数据中的异常，使用这样的判定树对类别未知的数据进行分类，分类的准确性不高。因此试图检测和减去这样的分支，检测和减去这些分支的过程被称为树剪枝。树剪枝方法用于处理过分适应数据问题。通常，这种方法使用统计度量，减去最不可靠的分支，这将导致较快的分类，提高树独立于训练数据正确分类的能力。

　　决策树常用的剪枝常用的简直方法有两种：预剪枝(Pre-Pruning)和后剪枝(Post-Pruning)。预剪枝是根据一些原则及早的停止树增长，如树的深度达到用户所要的深度、节点中样本个数少于用户指定个数、不纯度指标下降的最大幅度小于用户指定的幅度等。预剪枝的核心问题是如何事先指定树的最大深度，如果设置的最大深度不恰当，那么将会导致过于限制树的生长，使决策树的表达式规则趋于一般，不能更好地对新数据集进行分类和预测。除了事先限定决策树的最大深度之外，还有另外一个方法来实现预剪枝操作，那就是采用检验技术对当前结点对应的样本集合进行检验，如果该样本集合的样本数量已小于事先指定的最小允许值，那么停止该结点的继续生长，并将该结点变为叶子结点，否则可以继续扩展该结点。

　　后剪枝则是通过在完全生长的树上剪去分枝实现的，通过删除节点的分支来剪去树节点，可以使用的后剪枝方法有多种，比如：代价复杂性剪枝、最小误差剪枝、悲观误差剪枝等等。后剪枝操作是一个边修剪边检验的过程，一般规则标准是：在决策树的不断剪枝操作过程中，将原样本集合或新数据集合作为测试数据，检验决策树对测试数据的预测精度，并计算出相应的错误率，如果剪掉某个子树后的决策树对测试数据的预测精度或其他测度不降低，那么剪掉该子树。